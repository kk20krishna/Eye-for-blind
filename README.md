# Eye-for-blind
A deep learning model which can explain the contents of an image in the form of speech through caption generation with an attention mechanism.

This kind of model is a use-case for blind people so that they can understand any image with the help of speech. The caption generated through a CNN-RNN model will be converted to speech using a text to speech library.
This problem statement is an application of both deep learning and natural language processing. The features of an image will be extracted by a CNN-based encoder and this will be decoded by an RNN model.
The project is an extended application of Show, Attend and Tell: Neural Image Caption Generation with Visual Attention paper.

The dataset is taken from the Kaggle website and it consists of sentence-based image descriptions having a list of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events of the image.

## Training
<img width="1246" height="1898" alt="image" src="https://github.com/user-attachments/assets/f9b02fa1-fdf8-4749-97cb-bd50c8222f9d" />


## Generation
<img width="1237" height="1837" alt="image" src="https://github.com/user-attachments/assets/c5739465-4a7f-4ee3-ab22-70689795a9fc" />
